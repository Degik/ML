{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "interval = 0.4\n",
    "\n",
    "class NeuralNetworkLayer(nn.Module):\n",
    "    def __init__(self, hidden_size:int):\n",
    "        super(NeuralNetworkLayer, self).__init__()\n",
    "\n",
    "        #Layer 1 Input: 17 Output: 3\n",
    "        self.layer1 = nn.Linear(17, hidden_size)\n",
    "        #nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer1.weight, -interval, interval)\n",
    "        nn.init.xavier_uniform_(self.layer1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "        #nn.init.constant_(self.layer1.bias, 0.01)\n",
    "        #Layer 3 Input: 3 Output: 1\n",
    "        self.layer2 = nn.Linear(hidden_size, 2) # Two classes\n",
    "        #nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer2.weight, -interval, interval)\n",
    "        nn.init.xavier_uniform_(self.layer2.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.layer2.bias)\n",
    "        #nn.init.constant_(self.layer2.bias, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importMonkDataset(file_name:str) -> pd.DataFrame:\n",
    "    dataset = None\n",
    "    columns_name = [\"Y\"] + [f\"X{i}\" for i in range(1,7)] + [\"ID\"]\n",
    "    try:\n",
    "        dataset = pd.read_csv(file_name, sep=\" \", names=columns_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error | Parsing target dataset for validation!\")\n",
    "        print(e)\n",
    "    dataset.set_index('ID', inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def takeMonkInputDataset(dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    return dataset.iloc[:, 1:] #Return dataset without first and last column\n",
    " \n",
    "def takeMonkOutputDataset(dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    return dataset.iloc[:,[0]] #Return dataset with only first column\n",
    "\n",
    "\n",
    "def convert_x(x_train: np.ndarray):\n",
    "    dict_3 = {1: [1, 0, 0], 2: [0, 1, 0], 3: [0, 0, 1]}\n",
    "    dict_2 = {1: [1, 0], 2: [0, 1]}\n",
    "    dict_4 = {1: [1, 0, 0, 0], 2: [0, 1, 0, 0], 3: [0, 0, 1, 0], 4: [0, 0, 0, 1]}\n",
    "\n",
    "    new_y = []\n",
    "\n",
    "    for row in x_train:\n",
    "        new_row = []\n",
    "        for j, value in enumerate(row):\n",
    "            if j in [0, 1, 3]:\n",
    "                new_row.extend(dict_3.get(value))\n",
    "            elif j in [2, 5]:\n",
    "                new_row.extend(dict_2.get(value))\n",
    "            elif j == 4:\n",
    "                new_row.extend(dict_4.get(value))\n",
    "\n",
    "        new_y.append(new_row)\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN\n",
    "dataset_train = importMonkDataset(\"MONK/monks-2.train\")\n",
    "x_train = takeMonkInputDataset(dataset_train)\n",
    "y_train = takeMonkOutputDataset(dataset_train)\n",
    "\n",
    "#Convert data\n",
    "x_train = convert_x(x_train.to_numpy())\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "\n",
    "x_train = x_train.to(\"cuda:0\")\n",
    "y_train = y_train.to(\"cuda:0\")\n",
    "\n",
    "x_train = x_train.double()\n",
    "y_train = y_train.double()\n",
    "\n",
    "## TEST\n",
    "dataset_test = importMonkDataset(\"MONK/monks-2.test\")\n",
    "\n",
    "x_test = takeMonkInputDataset(dataset_test)\n",
    "y_test = takeMonkOutputDataset(dataset_test)\n",
    "x_test = x_test.to_numpy()\n",
    "\n",
    "#CONVERT MONK\n",
    "x_test = convert_x(x_test)\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "#CREATE TENSOR\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# MOVE TO GPU\n",
    "x_test = x_test.to(\"cuda:0\")\n",
    "y_test = y_test.to(\"cuda:0\")\n",
    "\n",
    "#SET TYPE DOUBLE\n",
    "x_test = x_test.double()\n",
    "y_test = y_test.double()\n",
    "\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=85, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=216, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 100\n",
    "results = []\n",
    "hidden_sizes = [2]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.2, 0.02, 0.002, 0.3, 0.03, 0.003, 0.04,0.004,0.05,0.005]\n",
    "\n",
    "for hidden_size, learning_rate in product(hidden_sizes,learning_rates):\n",
    "\n",
    "    net = NeuralNetworkLayer(hidden_size)\n",
    "\n",
    "    net = net.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #Values used for graphs\n",
    "    loss_values_train = []\n",
    "    accuracy_values_train = []\n",
    "    loss_values_test = []\n",
    "    accuracy_values_test = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "        accuracy = correct / total\n",
    "        accuracy_values_train.append(accuracy)\n",
    "        avg_loss = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values_train.append(avg_loss)\n",
    "\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY VAL\n",
    "        net.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                loss = criterion(outputs, batch_output)\n",
    "                total_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += batch_input.size(0)\n",
    "                correct += (predicted == batch_output).sum().item()\n",
    "            accuracy = correct / total\n",
    "            accuracy_values_test.append(accuracy)\n",
    "            avg_loss = total_loss / len(data_loader_test)\n",
    "            loss_values_test.append(avg_loss)\n",
    "        net.train()\n",
    "\n",
    "    #Create dir\n",
    "    path_name = f'GRID1_1/Monk2-{hidden_size}-{learning_rate}'\n",
    "    os.mkdir(path_name)\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values_train, label='Training Loss')\n",
    "    plt.plot(loss_values_test, label = 'Test loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Loss.png')\n",
    "    plt.clf()\n",
    "\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "    plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "\n",
    "    result = f'Hidden_size:{hidden_size}, Learning-rate:{learning_rate}, Accuracy: {accuracy_values_test[-1]:.4f}'\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"GRID1_1/resultsMONK2.txt\", 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(result + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.04\n",
    "hidden_units = 3\n",
    "num_epochs = 100\n",
    "\n",
    "net = NeuralNetworkLayer(hidden_units)\n",
    "\n",
    "net = net.to(\"cuda:0\")\n",
    "\n",
    "net = net.double()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#Model save \n",
    "torch.save(net, 'models/model1.pth')\n",
    "with open('models/model1_parameters.txt', 'w') as file:\n",
    "    file.write('Pesi layer1\\n')\n",
    "    file.write(str(net.layer1.weight.data) + '\\n')\n",
    "    file.write('Bias layer1\\n')\n",
    "    file.write(str(net.layer1.bias.data) + '\\n')\n",
    "    file.write('Pesi layer2\\n')\n",
    "    file.write(str(net.layer2.weight.data) + '\\n')\n",
    "    file.write('Bias layer1\\n')\n",
    "    file.write(str(net.layer2.bias.data) + '\\n')\n",
    "\n",
    "#Values used for graphs\n",
    "loss_values_train = []\n",
    "accuracy_values_train = []\n",
    "loss_values_test = []\n",
    "accuracy_values_test = []\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_input, batch_output in data_loader_train:\n",
    "        #Forward pass\n",
    "        outputs = net(batch_input)\n",
    "        #Training loss\n",
    "        batch_output_squeeze = batch_output.squeeze().long()\n",
    "        loss = criterion(outputs, batch_output_squeeze)\n",
    "        #Calculate total loss\n",
    "        total_loss += loss.item()\n",
    "        #Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_input.size(0)\n",
    "        correct += (predicted == batch_output_squeeze).sum().item()\n",
    "    accuracy = correct / total\n",
    "    accuracy_values_train.append(accuracy)\n",
    "    avg_loss = total_loss / len(data_loader_train)\n",
    "    #Add to list\n",
    "    loss_values_train.append(avg_loss)\n",
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #CALCULATE ACCURACY VAL\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            outputs = net(batch_input)\n",
    "            batch_output_squeeze = batch_output.squeeze().long()\n",
    "            loss = criterion(outputs, batch_output_squeeze)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output_squeeze).sum().item()\n",
    "        accuracy = correct / total\n",
    "        accuracy_values_test.append(accuracy)\n",
    "        avg_loss = total_loss / len(data_loader_test)\n",
    "        loss_values_test.append(avg_loss)\n",
    "    net.train()\n",
    "\n",
    "#Create dir\n",
    "path_name = f'models/Monk2-{hidden_units}-{learning_rate}'\n",
    "os.makedirs(path_name, exist_ok=True)\n",
    "#Save plot loss\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(loss_values_train, label='Training Loss')\n",
    "plt.plot(loss_values_test, label = 'Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path_name}/Loss.png')\n",
    "plt.clf()\n",
    "\n",
    "#Save plot accuracy\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_size, learning_rate, rate in product(hidden_sizes, learning_rates, rates_split):\n",
    "\n",
    "    #SPLIT IN TRAIN AND VAL\n",
    "    train_data, val_data, train_target, val_target = train_test_split(x_train, y_train, test_size=rate, random_state=40)\n",
    "\n",
    "    #CREATE TENSOR\n",
    "    train_data, val_data, train_target, val_target = torch.tensor(train_data), torch.tensor(val_data), torch.tensor(train_target), torch.tensor(val_target)\n",
    "\n",
    "    #MOVE TO GPU\n",
    "    train_data, val_data, train_target, val_target = train_data.to(\"cuda:0\"), val_data.to(\"cuda:0\"), train_target.to(\"cuda:0\"), val_target.to(\"cuda:0\")\n",
    "    \n",
    "    #SET TYPE DOUBLE\n",
    "    train_data, val_data, train_target, val_target = train_data.double(), val_data.double(), train_target.double(), val_target.double()\n",
    "\n",
    "    dataset_train = TensorDataset(train_data, train_target)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=10, shuffle=True)\n",
    "\n",
    "    dataset_val = TensorDataset(val_data, val_target)\n",
    "    data_loader_val = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "\n",
    "    #NET\n",
    "    net = NeuralNetworkLayer(hidden_size)\n",
    "    net = net.to(\"cuda:0\")\n",
    "    net = net.double()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    epochs = 390\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        #CALCULATE ACCURACY VAL\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_val:\n",
    "                outputs = net(batch_input)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += batch_input.size(0)\n",
    "                correct += (predicted == batch_output).sum().item()\n",
    "            accuracy = correct / total\n",
    "            accuracy_values.append(accuracy)\n",
    "\n",
    "        net.train()\n",
    "\n",
    "    #Create dir\n",
    "    path_name = f'GridPlot/test-{hidden_size}-{learning_rate}-{rate}'\n",
    "    os.mkdir(path_name)\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Loss.png')\n",
    "    plt.clf()\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            outputs = net(batch_input)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    result = f'Hidden_size:{hidden_size}, Learning-rate:{learning_rate}, Rate: {rate}, Accuracy: {accuracy:.4f}'\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"resultsMONK.txt\", 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(result + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
