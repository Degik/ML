{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class NeuralNetworkLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkLayer, self).__init__()\n",
    "\n",
    "        #Layer 1 Input: 10 Output: 100\n",
    "        self.layer1 = nn.Linear(10, 50)\n",
    "        #nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer1.weight, -0.7, 0.7)\n",
    "        #nn.init.constant_(self.layer1.bias, 0.01)\n",
    "        #Layer 2 Input: 50 Output: 100\n",
    "        self.layer2 = nn.Linear(50, 100)\n",
    "        #nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer2.weight, -0.7, 0.7)\n",
    "        #nn.init.constant_(self.layer2.bias, 0.01)\n",
    "        #Layer 3 Input: 100 Output: 25\n",
    "        self.layer3 = nn.Linear(100, 25)\n",
    "        #nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer3.weight, -0.7, 0.7)\n",
    "        #nn.init.constant_(self.layer3.bias, 0.01)\n",
    "        #Layer 4 Input: 25 Output: 3\n",
    "        self.layer4 = nn.Linear(25, 3)\n",
    "        #nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.uniform_(self.layer4.weight, -0.7, 0.7)\n",
    "        #nn.init.constant_(self.layer4.bias, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDatasetCupInput(file_name:str, blind:bool) -> pd.DataFrame:\n",
    "    dataset = []\n",
    "    try:\n",
    "        dataset = pd.read_csv(file_name, header=None, dtype=float)\n",
    "    except Exception as e:\n",
    "        print(\"Error | Can not read dataset cup for take input\")\n",
    "        exit(1)\n",
    "    if not blind:\n",
    "        dataset = dataset.iloc[:, :-3] # Remove 3 columns\n",
    "    columns_name = ['ID'] + [f'X{i}' for i in range(1,11)]\n",
    "    dataset.columns = columns_name\n",
    "    dataset.set_index('ID', inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def importDatasetCupOutput(file_name:str, blind:bool) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataset = pd.read_csv(file_name, header=None, dtype=float)\n",
    "    except Exception as e:\n",
    "        print(\"Error | Can not read dataset cup for take output\")\n",
    "        exit(1)\n",
    "    columns_list = ['ID', 'Y1', 'Y2', 'Y3']\n",
    "    if not blind: # Dataset with all inputs \n",
    "        indexes = [0, 10, 11, 12] # take the first and last 3 columns indexes\n",
    "        dataset = dataset.iloc[:, indexes]\n",
    "    dataset.columns = columns_list\n",
    "    dataset.set_index('ID', inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch Params\n",
    "results = []\n",
    "hidden_sizes = [8,7,6,5]\n",
    "learning_rates = [0.01, 0.001, 0.05, 0.005]\n",
    "rates_split = [0.2, 0.25, 0.3]\n",
    "\n",
    "#\n",
    "learning_rate = 0.01\n",
    "num_epochs = 300\n",
    "threshold = 0.00001\n",
    "\n",
    "## TRAIN\n",
    "x_train = importDatasetCupInput(\"CUP/ML-CUP23-TRAIN.csv\", blind=False)\n",
    "y_train = importDatasetCupOutput(\"CUP/ML-CUP23-TRAIN.csv\", blind=False)\n",
    "\n",
    "x_train = torch.tensor(x_train.to_numpy())\n",
    "y_train = torch.tensor(y_train.to_numpy())\n",
    "\n",
    "x_train = x_train.to(\"cuda:0\")\n",
    "y_train = y_train.to(\"cuda:0\")\n",
    "\n",
    "x_train = x_train.double()\n",
    "y_train = y_train.double()\n",
    "\n",
    "## TEST\n",
    "x_test = importDatasetCupInput(\"CUP/ML-CUP23-TEST-INPUT.csv\", blind=True)\n",
    "y_test = importDatasetCupOutput(\"CUP/ML-CUP23-TEST-TARGET.csv\", blind=True)\n",
    "\n",
    "x_test = torch.tensor(x_test.to_numpy())\n",
    "y_test = torch.tensor(y_test.to_numpy())\n",
    "\n",
    "x_test = x_test.to(\"cuda:0\")\n",
    "y_test = y_test.to(\"cuda:0\")\n",
    "\n",
    "x_test = x_test.double()\n",
    "y_test = y_test.double()\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=500, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=450, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = NeuralNetworkLayer()\n",
    "net = net.to(\"cuda:0\")\n",
    "net = net.double()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#Model save \n",
    "torch.save(net, 'modelsCup/model1.pth')\n",
    "with open('modelsCup/model1_parameters.txt', 'w') as file:\n",
    "    file.write('Pesi layer1\\n')\n",
    "    file.write(str(net.layer1.weight.data) + '\\n')\n",
    "    file.write('Bias layer1\\n')\n",
    "    file.write(str(net.layer1.bias.data) + '\\n')\n",
    "    file.write('Pesi layer2\\n')\n",
    "    file.write(str(net.layer2.weight.data) + '\\n')\n",
    "    file.write('Bias layer2\\n')\n",
    "    file.write(str(net.layer2.bias.data) + '\\n')\n",
    "    file.write('Pesi layer3\\n')\n",
    "    file.write(str(net.layer3.weight.data) + '\\n')\n",
    "    file.write('Bias layer3\\n')\n",
    "    file.write(str(net.layer3.bias.data) + '\\n')\n",
    "    file.write('Pesi layer4\\n')\n",
    "    file.write(str(net.layer4.weight.data) + '\\n')\n",
    "    file.write('Bias layer4\\n')\n",
    "    file.write(str(net.layer4.bias.data) + '\\n')\n",
    "\n",
    "#Values used for graphs\n",
    "loss_values_train = []\n",
    "accuracy_values_train = []\n",
    "loss_values_test = []\n",
    "accuracy_values_test = []\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_input, batch_output in data_loader_train:\n",
    "        #Forward pass\n",
    "        outputs = net(batch_input)\n",
    "        #Training loss\n",
    "        loss = criterion(outputs, batch_output)\n",
    "        #Calculate total loss\n",
    "        total_loss += loss.item()\n",
    "        #Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        difference = torch.abs(outputs - batch_output)\n",
    "        total += batch_input.size(0)\n",
    "        correct += torch.sum(difference < threshold)\n",
    "    accuracy = correct / total\n",
    "    accuracy_values_train.append(accuracy.item())\n",
    "    avg_loss = total_loss / len(data_loader_train)\n",
    "    #Add to list\n",
    "    loss_values_train.append(avg_loss)\n",
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    #CALCULATE ACCURACY VAL\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            outputs = net(batch_input)\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            difference = torch.abs(outputs - batch_output)\n",
    "            total += batch_input.size(0)\n",
    "            correct += torch.sum(difference < threshold)\n",
    "        accuracy = correct / total\n",
    "        accuracy_values_test.append(accuracy.item())\n",
    "        avg_loss = total_loss / len(data_loader_test)\n",
    "        loss_values_test.append(avg_loss)\n",
    "    net.train()\n",
    "\n",
    "#Create dir\n",
    "path_name = f'modelsCup/Cup-{threshold}-{learning_rate}'\n",
    "os.makedirs(path_name, exist_ok=True)\n",
    "#Save plot loss\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(loss_values_train, label='Training Loss')\n",
    "plt.plot(loss_values_test, label = 'Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path_name}/Loss.png')\n",
    "plt.clf()\n",
    "\n",
    "#Save plot accuracy\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(accuracy_values_train, label='Accuracy Train')\n",
    "plt.plot(accuracy_values_test, label='Accuracy Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate, rate in product(learning_rates, rates_split):\n",
    "\n",
    "    dataset_train = TensorDataset(train_data, train_target)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=100, shuffle=True)\n",
    "\n",
    "    dataset_val = TensorDataset(val_data, val_target)\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=100, shuffle=True)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    epochs = 390\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_val:\n",
    "                predicted = net(batch_input)\n",
    "                total += batch_input.size(0)\n",
    "                correct += (predicted == batch_output).sum().item()\n",
    "            accuracy = correct / total\n",
    "            accuracy_values.append(accuracy)\n",
    "\n",
    "        net.train()\n",
    "\n",
    "    #Create dir\n",
    "    path_name = f'CUP/GridPlot/test-{hidden_size}-{learning_rate}-{rate}'\n",
    "    os.mkdir(path_name)\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Loss.png')\n",
    "    plt.clf()\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            predicted = net(batch_input)\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    result = f'Hidden_size:{hidden_size}, Learning-rate:{learning_rate}, Rate: {rate} ,Accuracy: {accuracy:.4f}'\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"resultsCUP.txt\", 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(result + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
