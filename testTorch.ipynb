{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from itertools import product\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class NeuralNetworkLayer(nn.Module):\n",
    "    def __init__(self, hidden_size:int):\n",
    "        super(NeuralNetworkLayer, self).__init__()\n",
    "\n",
    "        #Layer 1 Input: 17 Output: 3\n",
    "        self.layer1 = nn.Linear(17, hidden_size)\n",
    "        #nn.init.kaiming_normal_(self.layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.uniform_(self.layer1.weight, -0.7, 0.7)\n",
    "        nn.init.constant_(self.layer1.bias, 0.01)\n",
    "        #Layer 3 Input: 3 Output: 1\n",
    "        self.layer2 = nn.Linear(hidden_size, 1)\n",
    "        #nn.init.kaiming_normal_(self.layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.uniform_(self.layer2.weight, -0.7, 0.7)\n",
    "        nn.init.constant_(self.layer2.bias, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def importMonkDataset(file_name:str) -> pd.DataFrame:\n",
    "    dataset = None\n",
    "    columns_name = [\"Y\"] + [f\"X{i}\" for i in range(1,7)] + [\"ID\"]\n",
    "    try:\n",
    "        dataset = pd.read_csv(file_name, sep=\" \", names=columns_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error | Parsing target dataset for validation!\")\n",
    "        print(e)\n",
    "    dataset.set_index('ID', inplace=True)\n",
    "    return dataset\n",
    "\n",
    "def takeMonkInputDataset(dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    return dataset.iloc[:, 1:] #Return dataset without first and last column\n",
    " \n",
    "def takeMonkOutputDataset(dataset:pd.DataFrame) -> pd.DataFrame:\n",
    "    return dataset.iloc[:,[0]] #Return dataset with only first column\n",
    "\n",
    "\n",
    "def convert_x(x_train: np.ndarray):\n",
    "    dict_3 = {1: [1, 0, 0], 2: [0, 1, 0], 3: [0, 0, 1]}\n",
    "    dict_2 = {1: [1, 0], 2: [0, 1]}\n",
    "    dict_4 = {1: [1, 0, 0, 0], 2: [0, 1, 0, 0], 3: [0, 0, 1, 0], 4: [0, 0, 0, 1]}\n",
    "\n",
    "    new_y = []\n",
    "\n",
    "    for row in x_train:\n",
    "        new_row = []\n",
    "        for j, value in enumerate(row):\n",
    "            if j in [0, 1, 3]:\n",
    "                new_row.extend(dict_3.get(value))\n",
    "            elif j in [2, 5]:\n",
    "                new_row.extend(dict_2.get(value))\n",
    "            elif j == 4:\n",
    "                new_row.extend(dict_4.get(value))\n",
    "\n",
    "        new_y.append(new_row)\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = importMonkDataset(\"MONK/monks-1.train\")\n",
    "x_train = takeMonkInputDataset(dataset_train)\n",
    "y_train = takeMonkOutputDataset(dataset_train)\n",
    "\n",
    "#Convert data\n",
    "x_train = convert_x(x_train.to_numpy())\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "x_train = torch.Tensor(x_train)\n",
    "y_train = torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Move data to gpu\n",
    "x_train = x_train.to(\"cuda:0\")\n",
    "y_train = y_train.to(\"cuda:0\")\n",
    "\n",
    "x_train = x_train.double()\n",
    "y_train = y_train.double()\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "data_loader = DataLoader(dataset_train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation: tanh, relu\n",
    "# solver: sgd, adam\n",
    "# lr: 0.01, 0.001, 0.05, 0.005\n",
    "# iter: 350, 400\n",
    "# alpha: 0.0001, 0.001, 0.0005, 0.005\n",
    "# layers: 2, 3, 4\n",
    "\n",
    "# parameters = {'activation':('tanh', 'relu'), \n",
    "#               'solver':('sgd', 'adam'), \n",
    "#               'lr':('0.01', '0.001', '0.05', '0.005'), \n",
    "#               'epochs': ('350', '400', '450'),\n",
    "#               'alpha' : ('0.0001', '0.001', '0.0005', '0.005'),\n",
    "#               'layers' : ('2', '3', '4')}\n",
    "\n",
    "#parameters = {'lr':('0.01', '0.001', '0.05', '0.005')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Net\n",
    "net = NeuralNetworkLayer(3)\n",
    "#\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "loss_values = []\n",
    "epochs = 390\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_input, batch_output in data_loader:\n",
    "        #Forward pass\n",
    "        outputs = net(batch_input)\n",
    "        #Training loss\n",
    "        loss = criterion(outputs, batch_output)\n",
    "        #Calculate accuracy\n",
    "        \n",
    "        #Outputs convert to binary 0 or 1 with P (Probability)\n",
    "        #outputsConv = (F.sigmoid(outputs) >= 0.5).float()\n",
    "        #_, predicted = torch.max(outputsConv.data, 1)\n",
    "        total_loss += loss.item()\n",
    "        #Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    #Add to list\n",
    "    loss_values.append(avg_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "#Update plot\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "epochs = 390\n",
    "for epoch in range(epochs):\n",
    "    #Forward pass\n",
    "    outputs = net(x_train)\n",
    "    #\n",
    "    outputs = (outputs > 0.5).float()\n",
    "    #Training loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    #Backward and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #Add to list\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "#Update plot\n",
    "display.clear_output(wait=True)\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, 'model390.pth')\n",
    "#outputsCl = outputs.clamp(0,1)\n",
    "#outputsB = (outputs > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = importMonkDataset(\"MONK/monks-1.test\")\n",
    "\n",
    "x_test = takeMonkInputDataset(dataset_test)\n",
    "y_test = takeMonkOutputDataset(dataset_test)\n",
    "x_test = x_test.to_numpy()\n",
    "\n",
    "x_test = convert_x(x_test)\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "x_test = x_test.to(\"cuda:0\")\n",
    "y_test = y_test.to(\"cuda:0\")\n",
    "\n",
    "x_test = x_test.double()\n",
    "y_test = y_test.double()\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "results = []\n",
    "hidden_sizes = [2,3,4,5]\n",
    "learning_rates = [0.01, 0.001, 0.05, 0.005]\n",
    "rates_split = [0.2, 0.25, 0.3]\n",
    "\n",
    "for hidden_size, learning_rate, rates in product(hidden_sizes, learning_rates, rates_split):\n",
    "\n",
    "    train_data, test_data, train_target, test_target = train_test_split()\n",
    "\n",
    "    net = NeuralNetworkLayer(hidden_size)\n",
    "    net = net.to(\"cuda:0\")\n",
    "    net = net.double()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_values = []\n",
    "    epochs = 390\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_output in data_loader:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        #Add to list\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "    #Save plot\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'GridPlot/{hidden_size}-{learning_rate}.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_test:\n",
    "            outputs = net(batch_input)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    result = f'Hidden_size:{hidden_size}, Learning-rate:{learning_rate}, Accuracy: {accuracy:.4f}'\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"results.txt\", 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(result + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = net(x_test)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    total = x_test.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden_size:5, Learning-rate:0.005, Rate: 0.3 ,Accuracy: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "results = []\n",
    "hidden_sizes = [2,3,4,5]\n",
    "learning_rates = [0.01, 0.001, 0.05, 0.005]\n",
    "rates_split = [0.2, 0.25, 0.3]\n",
    "\n",
    "## TRAIN\n",
    "dataset_train = importMonkDataset(\"MONK/monks-1.train\")\n",
    "x_train = takeMonkInputDataset(dataset_train)\n",
    "y_train = takeMonkOutputDataset(dataset_train)\n",
    "\n",
    "#Convert data\n",
    "x_train = convert_x(x_train.to_numpy())\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "#x_train = torch.Tensor(x_train)\n",
    "#y_train = torch.Tensor(y_train)\n",
    "\n",
    "## TEST\n",
    "dataset_val = importMonkDataset(\"MONK/monks-1.test\")\n",
    "\n",
    "x_val = takeMonkInputDataset(dataset_val)\n",
    "y_val = takeMonkOutputDataset(dataset_val)\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "x_val = convert_x(x_val)\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val)\n",
    "\n",
    "x_val = x_val.to(\"cuda:0\")\n",
    "y_val = y_val.to(\"cuda:0\")\n",
    "\n",
    "x_val = x_val.double()\n",
    "y_val = y_val.double()\n",
    "\n",
    "dataset_val = TensorDataset(x_val, y_val)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for hidden_size, learning_rate, rate in product(hidden_sizes, learning_rates, rates_split):\n",
    "\n",
    "    train_data, test_data, train_target, test_target = train_test_split(x_train, y_train, test_size=rate, random_state=40)\n",
    "\n",
    "    train_data, test_data, train_target, test_target = torch.tensor(train_data), torch.tensor(test_data), torch.tensor(train_target), torch.tensor(test_target)\n",
    "\n",
    "    train_data, test_data, train_target, test_target = train_data.to(\"cuda:0\"), test_data.to(\"cuda:0\"), train_target.to(\"cuda:0\"), test_target.to(\"cuda:0\")\n",
    "\n",
    "    train_data, test_data, train_target, test_target = train_data.double(), test_data.double(), train_target.double(), test_target.double()\n",
    "\n",
    "    dataset_train = TensorDataset(train_data, train_target)\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=10, shuffle=True)\n",
    "\n",
    "    dataset_test = TensorDataset(test_data, test_target)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=10, shuffle=True)\n",
    "\n",
    "    net = NeuralNetworkLayer(hidden_size)\n",
    "    net = net.to(\"cuda:0\")\n",
    "    net = net.double()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    epochs = 390\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_output in data_loader_train:\n",
    "            #Forward pass\n",
    "            outputs = net(batch_input)\n",
    "            #Training loss\n",
    "            loss = criterion(outputs, batch_output)\n",
    "            #Calculate total loss\n",
    "            total_loss += loss.item()\n",
    "            #Backward and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader_train)\n",
    "        #Add to list\n",
    "        loss_values.append(avg_loss)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_input, batch_output in data_loader_test:\n",
    "                outputs = net(batch_input)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                total += batch_input.size(0)\n",
    "                correct += (predicted == batch_output).sum().item()\n",
    "            accuracy = correct / total\n",
    "            accuracy_values.append(accuracy)\n",
    "\n",
    "        net.train()\n",
    "\n",
    "    #Create dir\n",
    "    path_name = f'GridPlot/test-{hidden_size}-{learning_rate}-{rate}'\n",
    "    os.mkdir(path_name)\n",
    "    #Save plot loss\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Loss.png')\n",
    "    plt.clf()\n",
    "    #Save plot accuracy\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(accuracy_values, label='Accuracy Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy for Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{path_name}/Accuracy-test.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_output in data_loader_val:\n",
    "            outputs = net(batch_input)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_input.size(0)\n",
    "            correct += (predicted == batch_output).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    result = f'Hidden_size:{hidden_size}, Learning-rate:{learning_rate}, Rate: {rate} ,Accuracy: {accuracy:.4f}'\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "with open(\"results.txt\", 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(result + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
